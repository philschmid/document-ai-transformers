{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document AI: Fine-tuning Donut for document-parsing using Hugging Face Transformers \n",
    "\n",
    "TODO: Add intro for blog why Donut and not LayoutLM and what is DocumentAI\n",
    "\n",
    "In this blog, you will learn how to fine-tune [Donut-base](https://huggingface.co/naver-clova-ix/donut-base) for document-understaind/document-parsing using Hugging Face Transformers.We are going use all of the great features from the Hugging Face ecosystem like model versioning and experiment tracking.\n",
    "\n",
    "\n",
    "As dataset we will use the [SROIE](https://github.com/zzzDavid/ICDAR-2019-SROIE) dataset a collection of 1000 scanned receipts including their OCR. More information for the dataset can be found at the [repository](https://github.com/zzzDavid/ICDAR-2019-SROIE).\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup Development Environment](#1-setup-habana-gaudi-instance)\n",
    "2. [Load SROIE dataset](#2-load-and-process-the-dataset) and process the \n",
    "3. [Prepare dataset for donut](#3-create-a-gauditrainer-and-an-run-single-hpu-fine-tuning)\n",
    "4. [Run training and evaluation](#4-run-distributed-data-parallel-training-with-gauditrainer)\n",
    "\n",
    "Before we can start make sure you have a [Hugging Face Account](https://huggingface.co/join) to save artifacts and experiments. \n",
    "\n",
    "## Quick intro: Document Understanding Transformer (Donut) by ClovaAI\n",
    "\n",
    "Document Understanding Transformer (Donut) is a new Transformer model for OCR-free document understanding. It doesn't require an OCR engine to process scanned documents, but is achieving state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing). \n",
    "Donut is a multimodal sequence-to-sequence model with a vision enconder ([Swin Transformer](https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/swin#overview)) and text decoder ([BART](https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/bart)). The encoder receives the images and computes it into a embedding, which is then passed to the decoder, which generates a sequence of tokens.\n",
    "\n",
    "![donut](../assets//donut.png)\n",
    "\n",
    "* Paper: https://arxiv.org/abs/2111.15664\n",
    "* Official repo:  https://github.com/clovaai/donut\n",
    "\n",
    "--- \n",
    "\n",
    "Now we know how Donut works, so lets get started. ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "Our first step is to install the Hugging Face Libraries including transformers and datasets. Running the following cell will install all the required packages.\n",
    "\n",
    "_Note: As the time of writing this Donut is not yet included in the PyPi version of Transformers, so we need it to install from the `main` branch. Donut will be added in version `4.22.0`._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git \n",
    "# !pip install -q \"transformers>=4.22.0\" # comment in when version is released\n",
    "!pip install -q datasets sentencepiece tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install git-fls for pushing model and logs to the hugging face hub\n",
    "!sudo apt-get install git-lfs --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. To be able to push our model to the Hub, you need to register on the [Hugging Face](https://huggingface.co/join). \n",
    "If you already have an account you can skip this step. \n",
    "After you have an account, we will use the `notebook_login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SROIE dataset\n",
    "\n",
    "As dataset we will use the [SROIE](https://github.com/zzzDavid/ICDAR-2019-SROIE) dataset a collection of 1000 scanned receipts including their OCR, more specifically we will use the dataset from task 2 \"Scanned Receipt OCR\". The available dataset on Hugging Face ([darentang/sroie](https://huggingface.co/datasets/darentang/sroie)) is not compatible with Donut. Thats why we will use the original dataset together with the `imagefolder` feature of `datasets` to load our dataset. Learn more about loading image data [here](https://huggingface.co/docs/datasets/v2.4.0/en/image_load#load-image-data).\n",
    "\n",
    "_Note: The test data for task2 is sadly not available. Meaning that we end up only with 624 images._\n",
    "\n",
    "As first we will cone the repository and extract the dataset into a separate folder and remove the unnecessary files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ICDAR-2019-SROIE'...\n",
      "Updating files: 100% (1980/1980), done.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "# clone repository\n",
    "git clone https://github.com/zzzDavid/ICDAR-2019-SROIE.git\n",
    "# copy data\n",
    "cp -r ICDAR-2019-SROIE/data ./\n",
    "# clean up\n",
    "rm -rf ICDAR-2019-SROIE\n",
    "rm -rf data/box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two folders inside the `data/` directory. One is containing the images of the receipts and the other is containing the OCR text. The nex step is to create a `metadata.json` file that contains the information about the images including the OCR-text. This is necessary for the `imagefolder` feature of `datasets`.\n",
    "\n",
    "The `metadata.json` should look at the end similar to the example below\n",
    "\n",
    "```json\n",
    "{\"file_name\": \"0001.png\", \"text\": \"This is a golden retriever playing with a ball\"}\n",
    "{\"file_name\": \"0002.png\", \"text\": \"A german shepherd\"}\n",
    "```\n",
    "\n",
    "In our example will `\"text\"` column contain the OCR text of the image, which will later be used for creating the Donut specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# define paths\n",
    "base_path = Path(\"data\")\n",
    "metadata_path = base_path.joinpath(\"key\")\n",
    "image_path = base_path.joinpath(\"img\")\n",
    "# define metadata list\n",
    "metadata_list = []\n",
    "\n",
    "# parse metadata\n",
    "for file_name in metadata_path.glob(\"*.json\"):\n",
    "  with open(file_name, \"r\") as json_file:\n",
    "    # load json file\n",
    "    data = json.load(json_file)\n",
    "    # create \"text\" column with json string\n",
    "    text = json.dumps(data)\n",
    "    # add to metadata list if image exists\n",
    "    if image_path.joinpath(f\"{file_name.stem}.jpg\").is_file():    \n",
    "      metadata_list.append({\"text\":text,\"file_name\":f\"{file_name.stem}.jpg\"})\n",
    "      # delete json file\n",
    "      \n",
    "# write jsonline file\n",
    "with open(image_path.joinpath('metadata.jsonl'), 'w') as outfile:\n",
    "    for entry in metadata_list:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "# remove old meta data\n",
    "shutil.rmtree(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Job! Now we can load the dataset using the `imagefolder` feature of `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# define paths\n",
    "base_path = Path(\"data\")\n",
    "metadata_path = base_path.joinpath(\"key\")\n",
    "image_path = base_path.joinpath(\"img\")\n",
    "# define metadata list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 627/627 [00:00<00:00, 227454.47it/s]\n",
      "Using custom data configuration default-ae7cbebba4bda4ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /home/ubuntu/.cache/huggingface/datasets/imagefolder/default-ae7cbebba4bda4ee/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files #0:   0%|          | 0/40 [00:00<?, ?obj/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading data files #2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 2211.11obj/s]\n",
      "Downloading data files #1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 1394.55obj/s]\n",
      "Downloading data files #0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 1417.78obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 580.24obj/s]\n",
      "Downloading data files #5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 608.42obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 579.46obj/s]\n",
      "Downloading data files #11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 1432.84obj/s]\n",
      "Downloading data files #12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 642.29obj/s]\n",
      "Downloading data files #7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 681.11obj/s]\n",
      "Downloading data files #8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 724.07obj/s]\n",
      "Downloading data files #9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 768.89obj/s]\n",
      "Downloading data files #10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 832.47obj/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading data files #13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 1049.07obj/s]\n",
      "Downloading data files #6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 1424.55obj/s]\n",
      "Downloading data files #14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 818.88obj/s]\n",
      "Downloading data files #15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:00<00:00, 5235.33obj/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/imagefolder/default-ae7cbebba4bda4ee/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff. Subsequent calls will reuse this data.\n",
      "Dataset has 626 images\n",
      "Dataset features are: dict_keys(['image', 'text'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=image_path, split=\"train\")\n",
    "\n",
    "print(f\"Dataset has {len(dataset)} images\")\n",
    "print(f\"Dataset features are: {dataset.features.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take a closer look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample is 47\n",
      "OCR text is {\"company\": \"RESTAURANT SIN DU\", \"date\": \"09/03/2018\", \"address\": \"K3-113, JL IBRAHIM SULTAN 80300 JOHOR BAHRU JOHOR\", \"total\": \"170.00\"}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=250x400 at 0x7F985C0806D0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_sample = random.randint(0, len(dataset))\n",
    "\n",
    "print(f\"Random sample is {random_sample}\")\n",
    "print(f\"OCR text is {dataset[random_sample]['text']}\")\n",
    "dataset[random_sample]['image'].resize((250,400))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset for donut\n",
    "\n",
    "As we learned in the introduction, Donut is a sequence-to-sequence model with a vision encoder and text decoder. When fine-tuning the model we want it to generate the `\"text\"` based on the image we pass it. Similar to NLP tasks, we have to tokenizer and preprocess the text. \n",
    "Before we can tokenize the text, we need to transform the JSON string into a Donut compatible document. \n",
    "\n",
    "**current JSON string**\n",
    "```json\n",
    "{\"company\": \"ADVANCO COMPANY\", \"date\": \"17/01/2018\", \"address\": \"NO 1&3, JALAN WANGSA DELIMA 12, WANGSA LINK, WANGSA MAJU, 53300 KUALA LUMPUR\", \"total\": \"7.00\"}\n",
    "```\n",
    "\n",
    "**donut document**\n",
    "```json\n",
    "<s></s><s_company>ADVANCO COMPANY</s_company><s_date>17/01/2018</s_date><s_address>NO 1&3, JALAN WANGSA DELIMA 12, WANGSA LINK, WANGSA MAJU, 53300 KUALA LUMPUR</s_address><s_total>7.00</s_total></s>\n",
    "```\n",
    "\n",
    "To easily create those documents the ClovaAI team has created a [json2token](https://github.com/clovaai/donut/blob/master/donut/model.py#L497) method, which we extract and then apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 286/626 [00:10<00:51,  6.60ex/s]"
     ]
    }
   ],
   "source": [
    "new_special_tokens = [] # new tokens which will be added to the tokenizer\n",
    "task_start_token = \"<s>\"  # start of task token\n",
    "eos_token = \"</s>\" # eos token of tokenizer\n",
    "\n",
    "def json2token(obj, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "    \"\"\"\n",
    "    Convert an ordered JSON object into a token sequence\n",
    "    \"\"\"\n",
    "    if type(obj) == dict:\n",
    "        if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "            return obj[\"text_sequence\"]\n",
    "        else:\n",
    "            output = \"\"\n",
    "            if sort_json_key:\n",
    "                keys = sorted(obj.keys(), reverse=True)\n",
    "            else:\n",
    "                keys = obj.keys()\n",
    "            for k in keys:\n",
    "                if update_special_tokens_for_json_key:\n",
    "                    new_special_tokens.append(fr\"<s_{k}>\") if fr\"<s_{k}>\" not in new_special_tokens else None\n",
    "                    new_special_tokens.append(fr\"</s_{k}>\") if fr\"</s_{k}>\" not in new_special_tokens else None\n",
    "                output += (\n",
    "                    fr\"<s_{k}>\"\n",
    "                    + json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                    + fr\"</s_{k}>\"\n",
    "                )\n",
    "            return output\n",
    "    elif type(obj) == list:\n",
    "        return r\"<sep/>\".join(\n",
    "            [json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "        )\n",
    "    else:\n",
    "        # excluded special tokens for now\n",
    "        obj = str(obj)\n",
    "        if f\"<{obj}/>\" in new_special_tokens:\n",
    "            obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "        return obj\n",
    "\n",
    "\n",
    "def preprocess_documents_for_donut(sample):\n",
    "    # create Donut-style input\n",
    "    text = json.loads(sample[\"text\"])\n",
    "    d_doc = task_start_token + json2token(text) + eos_token\n",
    "    # convert all images to RGB\n",
    "    image = sample[\"image\"].convert('RGB')\n",
    "    return {\"image\": image, \"text\": d_doc}\n",
    "\n",
    "proc_dataset = dataset.map(preprocess_documents_for_donut)\n",
    "\n",
    "print(f\"Sample: {proc_dataset[45]['text']}\")\n",
    "print(f\"New special tokens: {new_special_tokens + [task_start_token] + [eos_token]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenize our text and encode the images into tensors. Therefore we need to load `DonutProcessor`, add our new special tokens and adjust the size of the images when process from `[1920, 2560]` to `[960, 1280]` to need less memory and have faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_special_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m processor \u001b[39m=\u001b[39m DonutProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnaver-clova-ix/donut-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# add new special tokens to tokenizer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39madd_special_tokens({\u001b[39m\"\u001b[39m\u001b[39madditional_special_tokens\u001b[39m\u001b[39m\"\u001b[39m: new_special_tokens \u001b[39m+\u001b[39m [task_start_token] \u001b[39m+\u001b[39m [eos_token]})\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# we update some settings which differ from pretraining; namely the size of the images + no rotation required\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# resizing the image to smaller sizes from [1920, 2560] to [960,1280]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m processor\u001b[39m.\u001b[39mfeature_extractor\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m [\u001b[39m720\u001b[39m,\u001b[39m960\u001b[39m] \u001b[39m# should be (width, height)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_special_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor\n",
    "\n",
    "# Load processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# add new special tokens to tokenizer\n",
    "processor.tokenizer.add_special_tokens({\"additional_special_tokens\": new_special_tokens + [task_start_token] + [eos_token]})\n",
    "\n",
    "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
    "# resizing the image to smaller sizes from [1920, 2560] to [960,1280]\n",
    "processor.feature_extractor.size = [720,960] # should be (width, height)\n",
    "processor.feature_extractor.do_align_long_axis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can prepare our dataset, which we will use for the training later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 626/626 [02:11<00:00,  4.77ex/s]\n"
     ]
    }
   ],
   "source": [
    "def transform_and_tokenize(sample, processor=processor, split=\"train\", max_length=512, ignore_id=-1000):\n",
    "    # create tensor from image\n",
    "    try:\n",
    "        pixel_values = processor(\n",
    "            sample[\"image\"], random_padding=split == \"train\", return_tensors=\"pt\"\n",
    "        ).pixel_values.squeeze()\n",
    "    except Exception as e:\n",
    "        print(sample)\n",
    "        print(f\"Error: {e}\")\n",
    "        return {}\n",
    "        \n",
    "    # tokenize document\n",
    "    input_ids = processor.tokenizer(\n",
    "        sample[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = ignore_id  # model doesn't need to predict pad token\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"target_sequence\": sample[\"text\"]}\n",
    "\n",
    "# need at least 32-64GB of RAM to run this\n",
    "processed_dataset = proc_dataset.map(transform_and_tokenize,remove_columns=[\"image\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_dataset.save_to_disk(\"processed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DonutProcessor\n",
    "\n",
    "\n",
    "processed_dataset = load_from_disk(\"processed_dataset\")\n",
    "processor = DonutProcessor.from_pretrained(\"processor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run training and evaluation\n",
    "\n",
    "After we have processed our dataset, we can start training our model. Therefore we first need to load our model with the `VisionEncoderDecoderModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DonutSwinModel(\n",
       "    (embeddings): DonutSwinEmbeddings(\n",
       "      (patch_embeddings): DonutSwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DonutSwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): DonutSwinPatchMerging(\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): DonutSwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): DonutSwinPatchMerging(\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): DonutSwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): DonutSwinLayer(\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): DonutSwinAttention(\n",
       "                (self): DonutSwinSelfAttention(\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): DonutSwinSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DonutSwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): DonutSwinIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): DonutSwinOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (decoder): MBartForCausalLM(\n",
       "    (model): MBartDecoderWrapper(\n",
       "      (decoder): MBartDecoder(\n",
       "        (embed_tokens): Embedding(57533, 1024)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1538, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=57533, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, VisionEncoderDecoderConfig\n",
    "\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# resize embedding layer to match vocabulary size\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# adjust our image size and output sequence lengths\n",
    "model.config.encoder.image_size = processor.feature_extractor.size[::-1] # (height, width)\n",
    "model.config.decoder.max_length = len(max(processed_dataset[\"labels\"], key=len))\n",
    "\n",
    "# Add task token for decoder to start\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = processed_dataset.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values: torch.Size([1, 3, 1280, 960])\n",
      "Labels: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "pixel_values = torch.tensor(test_ds[0][\"pixel_values\"]).unsqueeze(0).to(device)\n",
    "labels = torch.tensor(test_ds[0][\"labels\"]).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Pixel values: {pixel_values.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [63,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [29,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1646755888534/work/aten/src/ATen/native/cuda/Indexing.cu:703: indexSelectLargeIndex: block: [109,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# the forward function automatically creates the correct decoder_input_ids\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss \u001b[39m=\u001b[39m model(pixel_values\u001b[39m=\u001b[39;49mpixel_values, labels\u001b[39m=\u001b[39;49mlabels)\u001b[39m.\u001b[39mloss\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bp3/home/ubuntu/document-ai-transformers/training/donut_sroie.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m loss\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:497\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m    493\u001b[0m         labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    496\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    498\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m    499\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m    500\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    501\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    502\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m    503\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    504\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    505\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    506\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    507\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    508\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_decoder,\n\u001b[1;32m    509\u001b[0m )\n\u001b[1;32m    511\u001b[0m \u001b[39m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[1;32m    512\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:1843\u001b[0m, in \u001b[0;36mMBartForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1840\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1842\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1843\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1844\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1845\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1846\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1847\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1848\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1849\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1850\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1851\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1852\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1853\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1854\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1855\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1856\u001b[0m )\n\u001b[1;32m   1858\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1860\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:1034\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[0;32m-> 1034\u001b[0m attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_decoder_attention_mask(\n\u001b[1;32m   1035\u001b[0m     attention_mask, input_shape, inputs_embeds, past_key_values_length\n\u001b[1;32m   1036\u001b[0m )\n\u001b[1;32m   1038\u001b[0m \u001b[39m# expand encoder attention mask\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m encoder_attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py:913\u001b[0m, in \u001b[0;36mMBartDecoder._prepare_decoder_attention_mask\u001b[0;34m(self, attention_mask, input_shape, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    911\u001b[0m combined_attention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39mif\u001b[39;00m input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 913\u001b[0m     combined_attention_mask \u001b[39m=\u001b[39m _make_causal_mask(\n\u001b[1;32m    914\u001b[0m         input_shape, inputs_embeds\u001b[39m.\u001b[39;49mdtype, past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length\n\u001b[1;32m    915\u001b[0m     )\u001b[39m.\u001b[39;49mto(inputs_embeds\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    917\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     expanded_attn_mask \u001b[39m=\u001b[39m _expand_mask(attention_mask, inputs_embeds\u001b[39m.\u001b[39mdtype, tgt_len\u001b[39m=\u001b[39minput_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(\n\u001b[1;32m    920\u001b[0m         inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    921\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(pixel_values=pixel_values, labels=labels).loss\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: cats-image/image\n",
      "Reusing dataset cats-image (/home/ubuntu/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 819.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "pixel_values = processor.feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "labels = processor.tokenizer(\n",
    "    \"an image of two cats chilling on a couch\",\n",
    "    return_tensors=\"pt\",\n",
    ").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values: torch.Size([1, 3, 1280, 960])\n",
      "Labels: torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pixel values: {pixel_values.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8497, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = model(pixel_values=pixel_values, labels=labels).loss\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
